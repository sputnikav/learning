{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n"
     ]
    }
   ],
   "source": [
    "import mnist_loader\n",
    "import numpy as np\n",
    "\n",
    "training_data, validation_data, test_data = mnist_loader.load_data_wrapper()\n",
    "print len(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1032\n"
     ]
    }
   ],
   "source": [
    "def sigmoid(z): # sigmoid function\n",
    "    return 1.0/(1.0+np.exp(-z))\n",
    "\n",
    "def sigmoid_prime(z): # sigmoid prime\n",
    "    return sigmoid(z)*(1.0-sigmoid(z))\n",
    "\n",
    "class Network:\n",
    "    def __init__(self, layer_sizes):\n",
    "        self.layers = []\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        self.layers = len(layer_sizes)\n",
    "        for input_size, output_size in zip(layer_sizes[:-1], layer_sizes[1:]):\n",
    "            weights = np.random.normal(size=(output_size, input_size))\n",
    "            self.weights.append(weights)\n",
    "            bias = np.random.normal(size=(output_size, 1))\n",
    "            self.biases.append(bias)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        a = x\n",
    "        for w, b in zip(self.weights, self.biases):\n",
    "            z = np.dot(w, a) + b\n",
    "            a = sigmoid(z)\n",
    "        return a\n",
    "    \n",
    "    def evaluate(self, test_data):\n",
    "        \"\"\"Return the number of test inputs for which the neural\n",
    "        network outputs the correct result. Note that the neural\n",
    "        network's output is assumed to be the index of whichever\n",
    "        neuron in the final layer has the highest activation.\"\"\"\n",
    "        test_results = [(np.argmax(self.forward(x)), y)\n",
    "                        for (x, y) in test_data]\n",
    "        return sum(int(x == y) for (x, y) in test_results)\n",
    "\n",
    "    def forward_backprop(self, x, y):\n",
    "        a = x\n",
    "        zs = []\n",
    "        activations = [x]\n",
    "        for w, b in zip(self.weights, self.biases):\n",
    "            z = np.dot(w, a) + b\n",
    "            zs.append(z)\n",
    "            a = sigmoid(z)\n",
    "            activations.append(a)\n",
    "        errors = []\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        delta = (activations[-1]-y)*sigmoid_prime(zs[-1]) # final detal_L\n",
    "        nabla_b[-1] = delta #important\n",
    "        nabla_w[-1] = np.dot(delta, activations[-2].transpose()) #important\n",
    "        for l in range(2, self.layers):\n",
    "            z = zs[-l]\n",
    "            delta = np.dot(self.weights[-l+1].transpose(), delta) * \\\n",
    "                sigmoid_prime(z)\n",
    "            nabla_b[-l] = delta\n",
    "            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
    "            \n",
    "        return (activations, nabla_w, nabla_b)\n",
    "    \n",
    "n = input_size = len(training_data[0][0])    \n",
    "network = Network([n, 30, 10])\n",
    "epoc = 0\n",
    "max_epoc = 1\n",
    "learning_rate = 0.01\n",
    "while epoc < max_epoc:\n",
    "    epoc += 1\n",
    "    error = 0\n",
    "    nabla_ws = []\n",
    "    nabla_bs = []\n",
    "    for x, y in training_data:\n",
    "        activations, nabla_w, nabla_b = network.forward_backprop(x, y)\n",
    "        nabla_ws.append(nabla_w)\n",
    "        nabla_bs.append(nabla_b)\n",
    "    delta_w = []\n",
    "    delta_b = []\n",
    "    for i in range(network.layers-1):\n",
    "        delta_w.append(sum([w[i] for w in nabla_ws]))\n",
    "        delta_b.append(sum([b[i] for b in nabla_bs]))\n",
    "    for i in range(network.layers-1):\n",
    "        network.weights[i] = network.weights[i] - learning_rate*delta_w[i]\n",
    "        network.biases[i] = network.biases[i] - learning_rate*delta_b[i]\n",
    "    \n",
    "    for x, y in training_data:\n",
    "        activations, nabla_w, nabla_b = network.forward_backprop(x, y)\n",
    "    print(network.evaluate(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
